# å®ç°æŒ‡å—ï¼šæ„å»º Node.js ä»£ç†

## å®ç°æ¦‚è¿°

æœ¬ç« æä¾›åŸºäº Node.js çš„è‡ªå»ºä»£ç†ï¼ˆProxyï¼‰å®Œæ•´å®ç°æ–¹æ¡ˆï¼Œç›®æ ‡æ˜¯ï¼š

- ğŸ”„ **åè®®æ¡¥æ¥**ï¼šæ”¯æŒ Chat â†” Responses åŒå‘è½¬æ¢
- ğŸš€ **æµå¼å¤„ç†**ï¼šå®Œæ•´çš„ SSE äº‹ä»¶æµè½¬å‘ä¸åˆæˆ
- ğŸŒ **å¤šå‚å•†æ”¯æŒ**ï¼šå¯æ‰©å±•è‡³å„ç§ä¸Šæ¸¸ LLM æœåŠ¡
- ğŸ› ï¸ **ç”Ÿäº§å°±ç»ª**ï¼šåŒ…å«é”™è¯¯å¤„ç†ã€ç›‘æ§ã€éƒ¨ç½²æ–¹æ¡ˆ

### å®ç°ç­–ç•¥é€‰æ‹©

| å®ç°æ¨¡å¼ | é€‚ç”¨åœºæ™¯ | å¤æ‚åº¦ | æ¨èåº¦ |
|----------|----------|--------|--------|
| **Chat ç›´é€š** | ä¸Šæ¸¸æ”¯æŒ Chat API | ä½ | â­â­â­ é¦–é€‰ |
| **Responses ç›´é€š** | ä¸Šæ¸¸æ”¯æŒ Responses API | ä½ | â­â­ å¦‚æœå¯ç”¨ |
| **Chat â†’ Responses æ¡¥æ¥** | éœ€è¦å®Œæ•´è¯­ä¹‰æ”¯æŒ | ä¸­ | â­â­ æŒ‰éœ€å®ç° |

## æŠ€æœ¯æ ˆä¸ä¾èµ–

### è¿è¡Œæ—¶ç¯å¢ƒ

```json
{
  "engines": {
    "node": ">=18.0.0"
  },
  "type": "module"
}
```

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- **å†…ç½® fetch**ï¼šNode 18+ åŸç”Ÿæ”¯æŒ
- **Web Streams**ï¼šæµå¼å¤„ç†æ ‡å‡† API
- **ES Modules**ï¼šç°ä»£æ¨¡å—ç³»ç»Ÿ

### é¡¹ç›®ä¾èµ–

```json
{
  "dependencies": {
    "express": "^4.19.2",      // Web æ¡†æ¶
    "cors": "^2.8.5",          // CORS æ”¯æŒ
    "helmet": "^7.1.0",        // å®‰å…¨å¤´
    "pino": "^8.17.0",         // é«˜æ€§èƒ½æ—¥å¿—
    "undici": "^6.6.0"         // å¯é€‰ï¼šé«˜æ€§èƒ½ HTTP å®¢æˆ·ç«¯
  },
  "devDependencies": {
    "tsx": "^4.7.0",           // TypeScript æ‰§è¡Œå™¨
    "typescript": "^5.4.0",    // TypeScript ç¼–è¯‘å™¨
    "@types/express": "^4.17.21",
    "@types/cors": "^2.8.17"
  }
}
```

### å¼€å‘å·¥å…·é“¾

```json
{
  "scripts": {
    "dev": "tsx watch src/server.ts",
    "build": "tsc && npm run copy-assets",
    "start": "node dist/server.js",
    "test": "tsx test/integration.test.ts",
    "copy-assets": "cp -r src/static dist/"
  }
}
```

## é¡¹ç›®ç»“æ„è®¾è®¡

```
codex-proxy/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.ts              # æœåŠ¡å™¨å…¥å£
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ chat.ts           # Chat Completions ç«¯ç‚¹
â”‚   â”‚   â”œâ”€â”€ responses.ts      # Responses ç«¯ç‚¹
â”‚   â”‚   â””â”€â”€ health.ts         # å¥åº·æ£€æŸ¥
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ sse.ts           # SSE å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ config.ts        # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ logger.ts        # æ—¥å¿—ç³»ç»Ÿ
â”‚   â”‚   â””â”€â”€ validation.ts    # è¯·æ±‚éªŒè¯
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”œâ”€â”€ chat-to-responses.ts    # Chat â†’ Responses è½¬æ¢
â”‚   â”‚   â”œâ”€â”€ responses-to-chat.ts    # Responses â†’ Chat è½¬æ¢
â”‚   â”‚   â””â”€â”€ tools-mapping.ts       # å·¥å…·è°ƒç”¨æ˜ å°„
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ auth.ts          # é‰´æƒä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ rate-limit.ts    # é™æµä¸­é—´ä»¶
â”‚   â”‚   â””â”€â”€ error-handler.ts # é”™è¯¯å¤„ç†
â”‚   â””â”€â”€ types/
â”‚       â”œâ”€â”€ openai.ts        # OpenAI API ç±»å‹
â”‚       â””â”€â”€ codex.ts         # Codex å†…éƒ¨ç±»å‹
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ integration/         # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ fixtures/           # æµ‹è¯•æ•°æ®
â”‚   â””â”€â”€ mocks/              # Mock æœåŠ¡
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ development.toml    # å¼€å‘é…ç½®
â”‚   â”œâ”€â”€ production.toml     # ç”Ÿäº§é…ç½®
â”‚   â””â”€â”€ local.toml.example  # é…ç½®æ¨¡æ¿
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile          # å®¹å™¨é•œåƒ
â”‚   â””â”€â”€ docker-compose.yml  # æœ¬åœ°å¼€å‘ç¯å¢ƒ
â””â”€â”€ docs/
    â”œâ”€â”€ api.md              # API æ–‡æ¡£
    â””â”€â”€ deployment.md       # éƒ¨ç½²æŒ‡å—
```

## æ ¸å¿ƒå®ç°

### æœåŠ¡å™¨å…¥å£

```typescript
// src/server.ts
import express from 'express';
import cors from 'cors';
import helmet from 'helmet';
import pino from 'pino';

import { loadConfig } from './lib/config.js';
import { createLogger } from './lib/logger.js';
import { authMiddleware } from './middleware/auth.js';
import { rateLimitMiddleware } from './middleware/rate-limit.js';
import { errorHandler } from './middleware/error-handler.js';

import chatRoutes from './routes/chat.js';
import responsesRoutes from './routes/responses.js';
import healthRoutes from './routes/health.js';

async function createServer() {
  const config = await loadConfig();
  const logger = createLogger(config.log);
  const app = express();

  // å®‰å…¨å’ŒåŸºç¡€ä¸­é—´ä»¶
  app.use(helmet());
  app.use(cors({
    origin: config.cors.allowedOrigins,
    credentials: true
  }));
  
  // è¯·æ±‚è§£æ
  app.use(express.json({ 
    limit: config.server.maxRequestSize 
  }));

  // è‡ªå®šä¹‰ä¸­é—´ä»¶
  app.use(authMiddleware(config.auth));
  app.use(rateLimitMiddleware(config.rateLimit));

  // è·¯ç”±æ³¨å†Œ
  app.use('/v1/chat', chatRoutes);
  app.use('/v1/responses', responsesRoutes);
  app.use('/health', healthRoutes);

  // é”™è¯¯å¤„ç†ï¼ˆå¿…é¡»åœ¨æœ€åï¼‰
  app.use(errorHandler(logger));

  return { app, logger, config };
}

async function main() {
  const { app, logger, config } = await createServer();
  
  const server = app.listen(config.server.port, () => {
    logger.info(
      `Codex Proxy listening on port ${config.server.port}`,
      { 
        environment: config.environment,
        upstreamUrl: config.upstream.baseUrl
      }
    );
  });

  // ä¼˜é›…å…³é—­
  process.on('SIGTERM', () => {
    logger.info('SIGTERM received, shutting down gracefully');
    server.close(() => {
      logger.info('Server closed');
      process.exit(0);
    });
  });
}

main().catch(console.error);
```

### é…ç½®ç®¡ç†

```typescript
// src/lib/config.ts
export interface ProxyConfig {
  environment: 'development' | 'production' | 'test';
  server: {
    port: number;
    maxRequestSize: string;
    timeout: number;
  };
  upstream: {
    baseUrl: string;
    apiKey?: string;
    supportsResponses: boolean;
    timeout: number;
    retries: number;
  };
  cors: {
    allowedOrigins: string[];
  };
  rateLimit: {
    windowMs: number;
    maxRequests: number;
  };
  auth: {
    enabled: boolean;
    apiKeys: string[];
  };
  log: {
    level: string;
    format: 'json' | 'pretty';
  };
}

export async function loadConfig(): Promise<ProxyConfig> {
  const environment = process.env.NODE_ENV || 'development';
  
  // åŸºç¡€é…ç½®
  const config: ProxyConfig = {
    environment: environment as ProxyConfig['environment'],
    server: {
      port: parseInt(process.env.PORT || '3000'),
      maxRequestSize: process.env.MAX_REQUEST_SIZE || '10mb',
      timeout: parseInt(process.env.SERVER_TIMEOUT || '300000'),
    },
    upstream: {
      baseUrl: process.env.UPSTREAM_BASE_URL || 'https://api.openai.com/v1',
      apiKey: process.env.UPSTREAM_API_KEY,
      supportsResponses: process.env.UPSTREAM_SUPPORTS_RESPONSES === 'true',
      timeout: parseInt(process.env.UPSTREAM_TIMEOUT || '300000'),
      retries: parseInt(process.env.UPSTREAM_RETRIES || '3'),
    },
    cors: {
      allowedOrigins: process.env.CORS_ORIGINS?.split(',') || ['*'],
    },
    rateLimit: {
      windowMs: parseInt(process.env.RATE_LIMIT_WINDOW || '60000'),
      maxRequests: parseInt(process.env.RATE_LIMIT_MAX || '100'),
    },
    auth: {
      enabled: process.env.AUTH_ENABLED === 'true',
      apiKeys: process.env.API_KEYS?.split(',') || [],
    },
    log: {
      level: process.env.LOG_LEVEL || 'info',
      format: (process.env.LOG_FORMAT as 'json' | 'pretty') || 'json',
    },
  };

  // é…ç½®éªŒè¯
  validateConfig(config);
  
  return config;
}

function validateConfig(config: ProxyConfig): void {
  if (!config.upstream.baseUrl) {
    throw new Error('UPSTREAM_BASE_URL is required');
  }
  
  if (config.auth.enabled && config.auth.apiKeys.length === 0) {
    throw new Error('API_KEYS required when AUTH_ENABLED=true');
  }
  
  // æ›´å¤šéªŒè¯é€»è¾‘...
}
```

### SSE å·¥å…·åº“

```typescript
// src/lib/sse.ts
import { Response } from 'express';

export class SSEWriter {
  private response: Response;
  private isConnected = true;

  constructor(response: Response) {
    this.response = response;
    this.setupSSE();
    this.setupErrorHandlers();
  }

  private setupSSE(): void {
    this.response.writeHead(200, {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache', 
      'Connection': 'keep-alive',
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Headers': 'Cache-Control'
    });
  }

  private setupErrorHandlers(): void {
    this.response.on('close', () => {
      this.isConnected = false;
    });

    this.response.on('error', (err) => {
      console.error('SSE connection error:', err);
      this.isConnected = false;
    });
  }

  writeData(data: any): boolean {
    if (!this.isConnected) return false;
    
    try {
      this.response.write(`data: ${JSON.stringify(data)}\n\n`);
      return true;
    } catch (err) {
      console.error('Failed to write SSE data:', err);
      this.isConnected = false;
      return false;
    }
  }

  writeComment(comment: string): boolean {
    if (!this.isConnected) return false;
    
    try {
      this.response.write(`: ${comment}\n\n`);
      return true;
    } catch (err) {
      this.isConnected = false;
      return false;
    }
  }

  writeDone(): boolean {
    if (!this.isConnected) return false;
    
    try {
      this.response.write('data: [DONE]\n\n');
      return true;
    } catch (err) {
      this.isConnected = false;
      return false;
    }
  }

  end(): void {
    if (this.isConnected) {
      this.response.end();
      this.isConnected = false;
    }
  }

  get connected(): boolean {
    return this.isConnected;
  }
}

// å¿ƒè·³ä¿æŒè¿æ¥
export class SSEHeartbeat {
  private timer?: NodeJS.Timeout;
  private writer: SSEWriter;

  constructor(writer: SSEWriter, intervalMs: number = 30000) {
    this.writer = writer;
    this.start(intervalMs);
  }

  private start(intervalMs: number): void {
    this.timer = setInterval(() => {
      if (this.writer.connected) {
        this.writer.writeComment('heartbeat');
      } else {
        this.stop();
      }
    }, intervalMs);
  }

  stop(): void {
    if (this.timer) {
      clearInterval(this.timer);
      this.timer = undefined;
    }
  }
}
```

### Chat Completions ç«¯ç‚¹

```typescript
// src/routes/chat.ts
import { Router, Request, Response } from 'express';
import { ProxyConfig } from '../lib/config.js';
import { SSEWriter, SSEHeartbeat } from '../lib/sse.js';
import { Logger } from 'pino';

const router = Router();

interface ChatCompletionsRequest {
  model: string;
  messages: any[];
  stream?: boolean;
  tools?: any[];
  [key: string]: any;
}

router.post('/completions', async (req: Request, res: Response) => {
  const config = req.app.get('config') as ProxyConfig;
  const logger = req.app.get('logger') as Logger;
  
  try {
    const requestBody = req.body as ChatCompletionsRequest;
    
    // å¼ºåˆ¶å¯ç”¨æµå¼
    requestBody.stream = true;
    
    // æ„å»ºä¸Šæ¸¸è¯·æ±‚
    const upstreamUrl = new URL('/chat/completions', config.upstream.baseUrl);
    
    // Azure ç‰¹æ®Šå¤„ç†
    if (isAzureEndpoint(config.upstream.baseUrl)) {
      upstreamUrl.searchParams.set('api-version', '2025-04-01-preview');
    }

    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
      'Accept': 'text/event-stream',
    };

    // é‰´æƒå¤´
    if (config.upstream.apiKey) {
      headers['Authorization'] = `Bearer ${config.upstream.apiKey}`;
    }

    // å‘èµ·ä¸Šæ¸¸è¯·æ±‚
    const upstreamResponse = await fetch(upstreamUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify(requestBody),
      signal: AbortSignal.timeout(config.upstream.timeout)
    });

    // é”™è¯¯å¤„ç†
    if (!upstreamResponse.ok) {
      await handleUpstreamError(upstreamResponse, res, logger);
      return;
    }

    // æµå¼è½¬å‘
    await streamChatResponse(upstreamResponse, res, logger);
    
  } catch (error) {
    logger.error({ error: error.message }, 'Chat endpoint error');
    res.status(500).json({
      error: {
        message: 'Internal server error',
        type: 'server_error'
      }
    });
  }
});

async function streamChatResponse(
  upstreamResponse: globalThis.Response,
  res: Response,
  logger: Logger
): Promise<void> {
  const writer = new SSEWriter(res);
  const heartbeat = new SSEHeartbeat(writer);
  
  try {
    const reader = upstreamResponse.body!.getReader();
    const decoder = new TextDecoder();
    let buffer = '';

    while (writer.connected) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });
      
      // å¤„ç†å®Œæ•´çš„ SSE å—
      let newlineIndex;
      while ((newlineIndex = buffer.indexOf('\n\n')) !== -1) {
        const chunk = buffer.slice(0, newlineIndex).trim();
        buffer = buffer.slice(newlineIndex + 2);
        
        if (chunk.startsWith('data: ')) {
          const data = chunk.slice(6);
          
          if (data === '[DONE]') {
            writer.writeDone();
            break;
          }
          
          try {
            const parsed = JSON.parse(data);
            writer.writeData(parsed);
          } catch (err) {
            logger.warn({ data }, 'Failed to parse SSE data');
          }
        }
      }
    }
  } catch (error) {
    logger.error({ error: error.message }, 'Stream processing error');
  } finally {
    heartbeat.stop();
    writer.end();
  }
}

async function handleUpstreamError(
  upstreamResponse: globalThis.Response,
  res: Response,
  logger: Logger
): Promise<void> {
  const statusCode = upstreamResponse.status;
  const statusText = upstreamResponse.statusText;
  
  // é€ä¼ é‡è¦å¤´
  const retryAfter = upstreamResponse.headers.get('retry-after');
  if (retryAfter) {
    res.set('Retry-After', retryAfter);
  }

  try {
    const errorBody = await upstreamResponse.text();
    logger.warn(
      { statusCode, statusText, errorBody },
      'Upstream error response'
    );
    
    res.status(statusCode).json(
      errorBody ? JSON.parse(errorBody) : {
        error: {
          message: statusText,
          type: 'upstream_error'
        }
      }
    );
  } catch (err) {
    res.status(statusCode).json({
      error: {
        message: statusText,
        type: 'upstream_error'
      }
    });
  }
}

function isAzureEndpoint(baseUrl: string): boolean {
  return baseUrl.includes('.openai.azure.com');
}

export default router;
```

### Responses ç«¯ç‚¹å®ç°

```typescript
// src/routes/responses.ts
import { Router, Request, Response } from 'express';
import { SSEWriter, SSEHeartbeat } from '../lib/sse.js';
import { chatToResponsesTransform } from '../transform/chat-to-responses.js';

const router = Router();

router.post('/', async (req: Request, res: Response) => {
  const config = req.app.get('config');
  const logger = req.app.get('logger');

  try {
    if (config.upstream.supportsResponses) {
      // ç›´é€šæ¨¡å¼ï¼šä¸Šæ¸¸åŸç”Ÿæ”¯æŒ Responses API
      await handleDirectResponsesMode(req, res, config, logger);
    } else {
      // æ¡¥æ¥æ¨¡å¼ï¼šChat â†’ Responses è½¬æ¢
      await handleBridgeMode(req, res, config, logger);
    }
  } catch (error) {
    logger.error({ error: error.message }, 'Responses endpoint error');
    res.status(500).json({
      error: {
        message: 'Internal server error',
        type: 'server_error'
      }
    });
  }
});

// ç›´é€šæ¨¡å¼ï¼šä¸Šæ¸¸æ”¯æŒ Responses
async function handleDirectResponsesMode(
  req: Request,
  res: Response,
  config: any,
  logger: any
): Promise<void> {
  const upstreamUrl = new URL('/responses', config.upstream.baseUrl);
  
  const headers = {
    'Content-Type': 'application/json',
    'Accept': 'text/event-stream',
    'OpenAI-Beta': 'responses=experimental'
  };

  if (config.upstream.apiKey) {
    headers['Authorization'] = `Bearer ${config.upstream.apiKey}`;
  }

  const upstreamResponse = await fetch(upstreamUrl, {
    method: 'POST',
    headers,
    body: JSON.stringify(req.body),
    signal: AbortSignal.timeout(config.upstream.timeout)
  });

  if (!upstreamResponse.ok) {
    await handleUpstreamError(upstreamResponse, res, logger);
    return;
  }

  // ç›´æ¥è½¬å‘ SSE æµ
  const writer = new SSEWriter(res);
  const heartbeat = new SSEHeartbeat(writer);

  try {
    const reader = upstreamResponse.body!.getReader();
    
    while (writer.connected) {
      const { done, value } = await reader.read();
      if (done) break;
      
      res.write(Buffer.from(value));
    }
  } finally {
    heartbeat.stop();
    writer.end();
  }
}

// æ¡¥æ¥æ¨¡å¼ï¼šChat â†’ Responses è½¬æ¢
async function handleBridgeMode(
  req: Request,
  res: Response,
  config: any,
  logger: any
): Promise<void> {
  // 1. å°† Responses è¯·æ±‚è½¬æ¢ä¸º Chat è¯·æ±‚
  const chatRequest = responsesToChatRequest(req.body);
  
  // 2. è°ƒç”¨ä¸Šæ¸¸ Chat API
  const upstreamUrl = new URL('/chat/completions', config.upstream.baseUrl);
  
  const headers = {
    'Content-Type': 'application/json',
    'Accept': 'text/event-stream'
  };

  if (config.upstream.apiKey) {
    headers['Authorization'] = `Bearer ${config.upstream.apiKey}`;
  }

  const upstreamResponse = await fetch(upstreamUrl, {
    method: 'POST',
    headers,
    body: JSON.stringify(chatRequest),
    signal: AbortSignal.timeout(config.upstream.timeout)
  });

  if (!upstreamResponse.ok) {
    await handleUpstreamError(upstreamResponse, res, logger);
    return;
  }

  // 3. å°† Chat æµè½¬æ¢ä¸º Responses äº‹ä»¶
  const writer = new SSEWriter(res);
  const heartbeat = new SSEHeartbeat(writer);

  try {
    await chatToResponsesTransform(upstreamResponse, writer, logger);
  } finally {
    heartbeat.stop();
    writer.end();
  }
}

// ç®€åŒ–ç‰ˆ Responses â†’ Chat è½¬æ¢
function responsesToChatRequest(responsesBody: any): any {
  const { model, instructions, input, tools } = responsesBody;

  // æ„å»º messages æ•°ç»„
  const messages: any[] = [];
  
  // ç³»ç»ŸæŒ‡ä»¤
  if (instructions) {
    messages.push({ role: 'system', content: instructions });
  }

  // å¤„ç†è¾“å…¥å†å²ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…å¤„ç†æ–‡æœ¬æ¶ˆæ¯ï¼‰
  if (Array.isArray(input)) {
    for (const item of input) {
      if (item?.type === 'message') {
        const content = extractTextContent(item.content);
        if (content) {
          messages.push({ 
            role: item.role || 'user', 
            content 
          });
        }
      }
      // TODO: å¤„ç†å·¥å…·è°ƒç”¨è¾“å‡ºç­‰å¤æ‚æƒ…å†µ
    }
  }

  return {
    model,
    messages,
    stream: true,
    tools: convertTooChatTools(tools)  // è½¬æ¢å·¥å…·æ ¼å¼
  };
}

function extractTextContent(content: any[]): string {
  if (!Array.isArray(content)) return '';
  
  return content
    .filter(c => c?.type === 'input_text' || c?.type === 'output_text')
    .map(c => c.text || '')
    .join('');
}

function convertTooChatTools(responsesTools: any[]): any[] {
  if (!Array.isArray(responsesTools)) return [];
  
  // åªä¿ç•™ function ç±»å‹å·¥å…·
  return responsesTools
    .filter(tool => tool?.type === 'function')
    .map(tool => ({
      type: 'function',
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.parameters
      }
    }));
}

export default router;
```

### Chat â†’ Responses äº‹ä»¶è½¬æ¢

```typescript
// src/transform/chat-to-responses.ts
import { SSEWriter } from '../lib/sse.js';

interface FunctionCallState {
  id?: string;
  name?: string;
  arguments: string;
}

export async function chatToResponsesTransform(
  upstreamResponse: globalThis.Response,
  writer: SSEWriter,
  logger: any
): Promise<void> {
  const reader = upstreamResponse.body!.getReader();
  const decoder = new TextDecoder();
  
  let buffer = '';
  let responseId = '';
  let assistantText = '';
  
  // å·¥å…·è°ƒç”¨çŠ¶æ€ï¼ˆæ”¯æŒå¹¶å‘è°ƒç”¨ï¼‰
  const functionCalls = new Map<number, FunctionCallState>();

  try {
    while (writer.connected) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });

      let newlineIndex;
      while ((newlineIndex = buffer.indexOf('\n\n')) !== -1) {
        const chunk = buffer.slice(0, newlineIndex).trim();
        buffer = buffer.slice(newlineIndex + 2);

        if (!chunk.startsWith('data: ')) continue;
        
        const data = chunk.slice(6).trim();
        
        if (data === '[DONE]') {
          // å…œåº•å®Œæˆäº‹ä»¶
          writer.writeData({
            type: 'response.completed',
            id: responseId || generateResponseId()
          });
          return;
        }

        try {
          const json = JSON.parse(data);
          responseId = json?.id || responseId;
          
          const choice = json?.choices?.[0];
          if (!choice) continue;

          const delta = choice.delta;
          const finishReason = choice.finish_reason;

          // å¤„ç†æ–‡æœ¬å¢é‡
          if (delta?.content) {
            assistantText += delta.content;
            writer.writeData({
              type: 'response.output_text.delta',
              delta: delta.content
            });
          }

          // å¤„ç†å·¥å…·è°ƒç”¨å¢é‡
          if (delta?.tool_calls) {
            procesToolCallselta(delta.tool_calls, functionCalls);
          }

          // å¤„ç†å®Œæˆè¯­ä¹‰
          if (finishReason === 'tool_calls') {
            // è¾“å‡ºæ‰€æœ‰èšåˆçš„å·¥å…·è°ƒç”¨
            for (const [, state] of functionCalls) {
              writer.writeData({
                type: 'response.output_item.done',
                item: {
                  type: 'function_call',
                  name: state.name || '',
                  arguments: state.arguments,
                  call_id: state.id || generateCallId()
                }
              });
            }

            // å®Œæˆäº‹ä»¶
            writer.writeData({
              type: 'response.completed',
              id: responseId
            });
            return;
          }

          if (finishReason === 'stop') {
            // è¾“å‡ºæœ€ç»ˆçš„ assistant æ¶ˆæ¯
            if (assistantText) {
              writer.writeData({
                type: 'response.output_item.done',
                item: {
                  type: 'message',
                  role: 'assistant',
                  content: [
                    {
                      type: 'output_text',
                      text: assistantText
                    }
                  ]
                }
              });
            }

            // å®Œæˆäº‹ä»¶
            writer.writeData({
              type: 'response.completed',
              id: responseId
            });
            return;
          }
        } catch (err) {
          logger.warn({ data, error: err.message }, 'Failed to parse Chat SSE data');
        }
      }
    }
  } catch (error) {
    logger.error({ error: error.message }, 'Chat to Responses transform error');
    
    // å‘é€é”™è¯¯äº‹ä»¶
    writer.writeData({
      type: 'response.failed',
      error: {
        message: 'Transform failed',
        type: 'transform_error'
      }
    });
  }
}

function procesToolCallselta(
  toolCalls: any[],
  functionCalls: Map<number, FunctionCallState>
): void {
  for (const toolCall of toolCalls) {
    const index = typeof toolCall.index === 'number' ? toolCall.index : 0;
    const state = functionCalls.get(index) || { arguments: '' };

    // æ›´æ–°çŠ¶æ€
    if (toolCall.id) {
      state.id = toolCall.id;
    }
    
    if (toolCall.function?.name) {
      state.name = toolCall.function.name;
    }
    
    if (toolCall.function?.arguments) {
      state.arguments += toolCall.function.arguments; // æ‹¼æ¥åˆ†ç‰‡
    }

    functionCalls.set(index, state);
  }
}

function generateResponseId(): string {
  return `resp_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
}

function generateCallId(): string {
  return `tc_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
}
```

## é«˜çº§ç‰¹æ€§

### é”™è¯¯å¤„ç†ä¸é‡è¯•

```typescript
// src/lib/retry.ts
export interface RetryOptions {
  maxAttempts: number;
  baseDelayMs: number;
  maxDelayMs: number;
  backoffMultiplier: number;
}

export class RetryHandler {
  constructor(private options: RetryOptions) {}

  async execute<T>(
    operation: () => Promise<T>,
    shouldRetry: (error: any) => boolean = this.defaultShouldRetry
  ): Promise<T> {
    let lastError: any;

    for (let attempt = 1; attempt <= this.options.maxAttempts; attempt++) {
      try {
        return await operation();
      } catch (error) {
        lastError = error;

        if (attempt === this.options.maxAttempts || !shouldRetry(error)) {
          throw error;
        }

        const delay = this.calculateDelay(attempt);
        await this.delay(delay);
      }
    }

    throw lastError;
  }

  private calculateDelay(attempt: number): number {
    const exponentialDelay = this.options.baseDelayMs * 
      Math.pow(this.options.backoffMultiplier, attempt - 1);
    
    return Math.min(exponentialDelay, this.options.maxDelayMs);
  }

  private defaultShouldRetry(error: any): boolean {
    // é‡è¯• 5xx é”™è¯¯å’Œç½‘ç»œé”™è¯¯
    if (error.status >= 500) return true;
    if (error.code === 'ECONNRESET' || error.code === 'ETIMEDOUT') return true;
    
    // 429 é™æµä¹Ÿé‡è¯•
    if (error.status === 429) return true;
    
    return false;
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const retryHandler = new RetryHandler({
  maxAttempts: 3,
  baseDelayMs: 1000,
  maxDelayMs: 10000,
  backoffMultiplier: 2
});

await retryHandler.execute(async () => {
  return fetch(upstreamUrl, requestOptions);
});
```

### ç›‘æ§ä¸æ—¥å¿—

```typescript
// src/lib/metrics.ts
export class MetricsCollector {
  private counters = new Map<string, number>();
  private histograms = new Map<string, number[]>();

  incrementCounter(name: string, value: number = 1): void {
    const current = this.counters.get(name) || 0;
    this.counters.set(name, current + value);
  }

  recordDuration(name: string, duration: number): void {
    const values = this.histograms.get(name) || [];
    values.push(duration);
    this.histograms.set(name, values);
  }

  getSnapshot(): MetricsSnapshot {
    const snapshot: MetricsSnapshot = {
      counters: Object.fromEntries(this.counters),
      histograms: {}
    };

    for (const [name, values] of this.histograms) {
      snapshot.histograms[name] = {
        count: values.length,
        sum: values.reduce((a, b) => a + b, 0),
        min: Math.min(...values),
        max: Math.max(...values),
        avg: values.reduce((a, b) => a + b, 0) / values.length,
        p95: this.percentile(values, 0.95),
        p99: this.percentile(values, 0.99)
      };
    }

    return snapshot;
  }

  private percentile(values: number[], p: number): number {
    const sorted = [...values].sort((a, b) => a - b);
    const index = Math.ceil(sorted.length * p) - 1;
    return sorted[index];
  }
}

// ä¸­é—´ä»¶é›†æˆ
export function metricsMiddleware(metrics: MetricsCollector) {
  return (req: Request, res: Response, next: Function) => {
    const startTime = Date.now();

    res.on('finish', () => {
      const duration = Date.now() - startTime;
      
      metrics.incrementCounter('http_requests_total');
      metrics.incrementCounter(`http_requests_${res.statusCode}`);
      metrics.recordDuration('http_request_duration_ms', duration);
    });

    next();
  };
}
```

### å¥åº·æ£€æŸ¥

```typescript
// src/routes/health.ts
import { Router } from 'express';

const router = Router();

router.get('/', async (req, res) => {
  const config = req.app.get('config');
  
  try {
    // æ£€æŸ¥ä¸Šæ¸¸è¿æ¥
    const upstreamHealth = await checkUpstreamHealth(config.upstream);
    
    // æ£€æŸ¥ç³»ç»Ÿèµ„æº
    const systemHealth = checkSystemHealth();
    
    const health = {
      status: 'healthy',
      timestamp: new Date().toISOString(),
      version: process.env.npm_package_version || 'unknown',
      uptime: process.uptime(),
      upstream: upstreamHealth,
      system: systemHealth
    };

    res.json(health);
  } catch (error) {
    res.status(503).json({
      status: 'unhealthy',
      timestamp: new Date().toISOString(),
      error: error.message
    });
  }
});

router.get('/ready', async (req, res) => {
  // å°±ç»ªæ¢é’ˆï¼šæ£€æŸ¥æœåŠ¡æ˜¯å¦å‡†å¤‡å¥½æ¥å—è¯·æ±‚
  try {
    const config = req.app.get('config');
    await checkUpstreamHealth(config.upstream);
    res.status(200).json({ status: 'ready' });
  } catch (error) {
    res.status(503).json({ status: 'not ready', error: error.message });
  }
});

router.get('/live', (req, res) => {
  // å­˜æ´»æ¢é’ˆï¼šæ£€æŸ¥æœåŠ¡æ˜¯å¦è¿˜æ´»ç€
  res.status(200).json({ status: 'alive' });
});

async function checkUpstreamHealth(upstreamConfig: any): Promise<any> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), 5000);

  try {
    const response = await fetch(
      new URL('/models', upstreamConfig.baseUrl),
      {
        method: 'GET',
        headers: upstreamConfig.apiKey ? {
          'Authorization': `Bearer ${upstreamConfig.apiKey}`
        } : {},
        signal: controller.signal
      }
    );

    return {
      status: response.ok ? 'healthy' : 'degraded',
      statusCode: response.status,
      responseTime: Date.now() - Date.now() // ç®€åŒ–ç‰ˆ
    };
  } finally {
    clearTimeout(timeoutId);
  }
}

function checkSystemHealth() {
  const memUsage = process.memoryUsage();
  
  return {
    memory: {
      used: Math.round(memUsage.heapUsed / 1024 / 1024),
      total: Math.round(memUsage.heapTotal / 1024 / 1024),
      external: Math.round(memUsage.external / 1024 / 1024)
    },
    cpu: process.cpuUsage(),
    eventLoop: {
      // ç®€åŒ–çš„äº‹ä»¶å¾ªç¯å»¶è¿Ÿæ£€æµ‹
      lag: 0 // å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„æ£€æµ‹
    }
  };
}

export default router;
```

## éƒ¨ç½²ä¸è¿ç»´

### Docker åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM node:20-alpine AS builder

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY package*.json ./
RUN npm ci --only=production

# æ„å»ºåº”ç”¨
COPY . .
RUN npm run build

# ç”Ÿäº§é•œåƒ
FROM node:20-alpine AS runtime

# é root ç”¨æˆ·
RUN addgroup -g 1001 -S nodejs && \
    adduser -S proxy -u 1001

WORKDIR /app

# å¤åˆ¶æ„å»ºäº§ç‰©
COPY --from=builder --chown=proxy:nodejs /app/node_modules ./node_modules
COPY --from=builder --chown=proxy:nodejs /app/dist ./dist
COPY --from=builder --chown=proxy:nodejs /app/package.json ./

USER proxy

EXPOSE 3000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD node -e "fetch('http://localhost:3000/health').then(r=>r.ok?process.exit(0):process.exit(1))"

CMD ["node", "dist/server.js"]
```

### Docker Compose å¼€å‘ç¯å¢ƒ

```yaml
# docker-compose.yml
version: '3.8'

services:
  codex-proxy:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - LOG_LEVEL=debug
      - UPSTREAM_BASE_URL=http://mock-llm:8000/v1
      - UPSTREAM_SUPPORTS_RESPONSES=false
    depends_on:
      - mock-llm
    volumes:
      - ./config:/app/config:ro

  mock-llm:
    image: mock-server:latest
    ports:
      - "8000:8000"
    volumes:
      - ./test/fixtures:/app/fixtures:ro

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
```

### Kubernetes éƒ¨ç½²

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: codex-proxy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: codex-proxy
  template:
    metadata:
      labels:
        app: codex-proxy
    spec:
      containers:
      - name: proxy
        image: codex-proxy:latest
        ports:
        - containerPort: 3000
        env:
        - name: NODE_ENV
          value: "production"
        - name: UPSTREAM_BASE_URL
          valueFrom:
            secretKeyRef:
              name: codex-config
              key: upstream-url
        - name: UPSTREAM_API_KEY
          valueFrom:
            secretKeyRef:
              name: codex-config
              key: api-key
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 3000
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: codex-proxy-service
spec:
  selector:
    app: codex-proxy
  ports:
  - protocol: TCP
    port: 80
    targetPort: 3000
  type: LoadBalancer
```

### ç›‘æ§é…ç½®

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'codex-proxy'
    static_configs:
      - targets: ['codex-proxy:3000']
    metrics_path: '/metrics'
    scrape_interval: 5s

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
```

## æ€§èƒ½ä¼˜åŒ–

### è¿æ¥æ± ä¼˜åŒ–

```typescript
// src/lib/http-client.ts
import { Agent } from 'undici';

export class OptimizedHttpClient {
  private agent: Agent;

  constructor() {
    this.agent = new Agent({
      keepAliveTimeout: 30000,
      keepAliveMaxTimeout: 600000,
      maxRedirections: 3,
      connect: {
        timeout: 10000,
        keepAlive: true
      }
    });
  }

  async fetch(url: string | URL, options: any = {}): Promise<Response> {
    return fetch(url, {
      ...options,
      dispatcher: this.agent
    });
  }

  destroy(): void {
    this.agent.close();
  }
}
```

### ç¼“å­˜ç­–ç•¥

```typescript
// src/lib/cache.ts
export interface CacheEntry<T> {
  value: T;
  expiresAt: number;
}

export class MemoryCache<T> {
  private cache = new Map<string, CacheEntry<T>>();
  private maxSize: number;

  constructor(maxSize: number = 1000) {
    this.maxSize = maxSize;
  }

  set(key: string, value: T, ttlMs: number): void {
    // LRU æ¸…ç†
    if (this.cache.size >= this.maxSize) {
      const firstKey = this.cache.keys().next().value;
      this.cache.delete(firstKey);
    }

    this.cache.set(key, {
      value,
      expiresAt: Date.now() + ttlMs
    });
  }

  get(key: string): T | null {
    const entry = this.cache.get(key);
    if (!entry) return null;

    if (entry.expiresAt < Date.now()) {
      this.cache.delete(key);
      return null;
    }

    return entry.value;
  }

  clear(): void {
    this.cache.clear();
  }
}

// æ¨¡å‹åˆ—è¡¨ç¼“å­˜ç¤ºä¾‹
const modelCache = new MemoryCache<any[]>();

export async function getCachedModels(upstreamUrl: string): Promise<any[]> {
  const cacheKey = `models:${upstreamUrl}`;
  
  let models = modelCache.get(cacheKey);
  if (models) return models;

  // ä»ä¸Šæ¸¸è·å–
  const response = await fetch(new URL('/models', upstreamUrl));
  models = await response.json();
  
  // ç¼“å­˜ 5 åˆ†é’Ÿ
  modelCache.set(cacheKey, models, 5 * 60 * 1000);
  
  return models;
}
```

---

## ä¸‹ä¸€æ­¥
- **[æµ‹è¯•éªŒè¯](./06-testing-validation.md)**ï¼šéªŒè¯ä»£ç†å®ç°çš„æ­£ç¡®æ€§
- **[å·¥å…·é›†æˆ](./04-tools-integration.md)**ï¼šä¸ºä»£ç†æ·»åŠ å·¥å…·æ”¯æŒ
- **[é…ç½®æŒ‡å—](./03-configuration-guide.md)**ï¼šä¼˜åŒ– Codex é…ç½®

è¿™ä»½å®ç°æŒ‡å—æä¾›äº†æ„å»ºç”Ÿäº§çº§ Node.js ä»£ç†çš„å®Œæ•´æ–¹æ¡ˆï¼Œæ¶µç›–äº†ä»åŸºç¡€åŠŸèƒ½åˆ°é«˜çº§ç‰¹æ€§çš„å„ä¸ªæ–¹é¢ã€‚é€šè¿‡è¿™ä¸ªå®ç°ï¼Œä½ å¯ä»¥ä¸º Codex æ„å»ºå¯é ã€é«˜æ€§èƒ½çš„ LLM ä»£ç†æœåŠ¡ã€‚